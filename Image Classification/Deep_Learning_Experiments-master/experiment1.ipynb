{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "uBeeyVyOwOOd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f0dd2aff-155e-4e60-8356-3a5dc6f0249b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/542.0 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m307.2/542.0 kB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m542.0/542.0 kB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.1/84.1 kB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m10.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m11.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m401.2/401.2 kB\u001b[0m \u001b[31m15.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install -q transformers datasets evaluate"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kRY5kcmWwOOe"
      },
      "source": [
        "Import Libraries and Load Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hgv20ia0wOOe",
        "outputId": "90d6042f-a91d-4286-cf0e-972f1c7cbd8f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
            "170498071/170498071 [==============================] - 4s 0us/step\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "from keras.datasets import cifar10\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
        "from keras.optimizers import Adam\n",
        "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
        "from sklearn.metrics import accuracy_score\n",
        "from keras.utils import to_categorical\n",
        "\n",
        "# Load CIFAR-10 dataset\n",
        "(X_train, Y_train), (X_test, Y_test) = cifar10.load_data()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "afX3k-1zwOOf"
      },
      "source": [
        "Data Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "JdMQ_ss9wOOg"
      },
      "outputs": [],
      "source": [
        "# Normalize data\n",
        "X_train = X_train.astype('float32') / 255.0\n",
        "X_test = X_test.astype('float32') / 255.0\n",
        "\n",
        "# Convert class vectors to binary class matrices\n",
        "Y_train = to_categorical(Y_train, 10)\n",
        "Y_test = to_categorical(Y_test, 10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4d6aVc9pwOOg"
      },
      "source": [
        "Define the Model Architecture"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jO2rShenwOOg"
      },
      "source": [
        "Configuration 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "kC82j3PswOOh"
      },
      "outputs": [],
      "source": [
        "model1 = Sequential([\n",
        "    Conv2D(32, (3, 3), activation='relu', input_shape=(32, 32, 3)),\n",
        "    Conv2D(32, (3, 3), activation='relu'),\n",
        "    MaxPooling2D(pool_size=(2, 2)),\n",
        "    Dropout(0.25),\n",
        "\n",
        "    Conv2D(64, (3, 3), activation='relu'),\n",
        "    Conv2D(64, (3, 3), activation='relu'),\n",
        "    MaxPooling2D(pool_size=(2, 2)),\n",
        "    Dropout(0.25),\n",
        "\n",
        "    Flatten(),\n",
        "    Dense(512, activation='relu'),\n",
        "    Dropout(0.5),\n",
        "    Dense(10, activation='softmax')\n",
        "])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PyOGc8QqwOOi"
      },
      "source": [
        "Configuration 2 - Change in Layer Depth and Filters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "nWw_nkGgwOOj"
      },
      "outputs": [],
      "source": [
        "model2 = Sequential([\n",
        "    Conv2D(48, (3, 3), activation='relu', input_shape=(32, 32, 3)),\n",
        "    Conv2D(48, (3, 3), activation='relu'),\n",
        "    MaxPooling2D(pool_size=(2, 2)),\n",
        "    Dropout(0.2),\n",
        "\n",
        "    Conv2D(96, (3, 3), activation='relu'),\n",
        "    Conv2D(96, (3, 3), activation='relu'),\n",
        "    MaxPooling2D(pool_size=(2, 2)),\n",
        "    Dropout(0.3),\n",
        "\n",
        "    Flatten(),\n",
        "    Dense(1024, activation='relu'),\n",
        "    Dropout(0.4),\n",
        "    Dense(10, activation='softmax')\n",
        "])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G_f7ir44wOOj"
      },
      "source": [
        "Configuration 3 - Introduction of Batch Normalization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "BxR1olyhwOOk"
      },
      "outputs": [],
      "source": [
        "from keras.layers import BatchNormalization\n",
        "\n",
        "model3 = Sequential([\n",
        "    Conv2D(32, (3, 3), activation='relu', input_shape=(32, 32, 3)),\n",
        "    BatchNormalization(),\n",
        "    Conv2D(32, (3, 3), activation='relu'),\n",
        "    BatchNormalization(),\n",
        "    MaxPooling2D(pool_size=(2, 2)),\n",
        "    Dropout(0.25),\n",
        "\n",
        "    Conv2D(64, (3, 3), activation='relu'),\n",
        "    BatchNormalization(),\n",
        "    Conv2D(64, (3, 3), activation='relu'),\n",
        "    BatchNormalization(),\n",
        "    MaxPooling2D(pool_size=(2, 2)),\n",
        "    Dropout(0.25),\n",
        "\n",
        "    Flatten(),\n",
        "    Dense(512, activation='relu'),\n",
        "    Dropout(0.5),\n",
        "    Dense(10, activation='softmax')\n",
        "])\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NOF86txtwOOk"
      },
      "source": [
        "Configuration 4 - Different Activation Functions and Optimizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "dV9wqFg-wOOl"
      },
      "outputs": [],
      "source": [
        "model4 = Sequential([\n",
        "    Conv2D(32, (3, 3), activation='elu', input_shape=(32, 32, 3)),\n",
        "    Conv2D(32, (3, 3), activation='elu'),\n",
        "    MaxPooling2D(pool_size=(2, 2)),\n",
        "    Dropout(0.25),\n",
        "\n",
        "    Conv2D(64, (3, 3), activation='elu'),\n",
        "    Conv2D(64, (3, 3), activation='elu'),\n",
        "    MaxPooling2D(pool_size=(2, 2)),\n",
        "    Dropout(0.25),\n",
        "\n",
        "    Flatten(),\n",
        "    Dense(512, activation='elu'),\n",
        "    Dropout(0.5),\n",
        "    Dense(10, activation='softmax')\n",
        "])\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GLvei6IUwOOl"
      },
      "source": [
        "Configuration 5 - Varying Dropout Rates"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "pgQEuTo3wOOl"
      },
      "outputs": [],
      "source": [
        "model5= Sequential([\n",
        "    Conv2D(32, (3, 3), activation='relu', input_shape=(32, 32, 3)),\n",
        "    Conv2D(32, (3, 3), activation='relu'),\n",
        "    MaxPooling2D(pool_size=(2, 2)),\n",
        "    Dropout(0.2),\n",
        "\n",
        "    Conv2D(64, (3, 3), activation='relu'),\n",
        "    Conv2D(64, (3, 3), activation='relu'),\n",
        "    MaxPooling2D(pool_size=(2, 2)),\n",
        "    Dropout(0.35),\n",
        "\n",
        "    Flatten(),\n",
        "    Dense(512, activation='relu'),\n",
        "    Dropout(0.45),\n",
        "    Dense(10, activation='softmax')\n",
        "])\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rpm2zhVpwOOl"
      },
      "source": [
        "Configuration 6 - Improving on Configuration 1 (Increased Learning Rate)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "B7UndCgcwOOl"
      },
      "outputs": [],
      "source": [
        "model6 = Sequential([\n",
        "    Conv2D(32, (3, 3), activation='relu', input_shape=(32, 32, 3)),\n",
        "    Conv2D(32, (3, 3), activation='relu'),\n",
        "    MaxPooling2D(pool_size=(2, 2)),\n",
        "    Dropout(0.25),\n",
        "\n",
        "    Conv2D(64, (3, 3), activation='relu'),\n",
        "    Conv2D(64, (3, 3), activation='relu'),\n",
        "    MaxPooling2D(pool_size=(2, 2)),\n",
        "    Dropout(0.25),\n",
        "\n",
        "    Flatten(),\n",
        "    Dense(512, activation='relu'),\n",
        "    Dropout(0.5),\n",
        "    Dense(10, activation='softmax')\n",
        "])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6xWZIwDNwOOm"
      },
      "source": [
        "Configuration 7 - Improving on Configuration 6 (Increased Learning Rate, Data Augmentation)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "Fq1PGHcawOOm"
      },
      "outputs": [],
      "source": [
        "\n",
        "model7 = Sequential([\n",
        "    Conv2D(64, (3, 3), padding='same', activation='relu', input_shape=(32, 32, 3)),\n",
        "    Conv2D(64, (3, 3), padding='same', activation='relu'),\n",
        "    MaxPooling2D(pool_size=(2, 2)),\n",
        "    Dropout(0.3),\n",
        "\n",
        "    Conv2D(128, (3, 3), padding='same', activation='relu'),\n",
        "    Conv2D(128, (3, 3), padding='same', activation='relu'),\n",
        "    MaxPooling2D(pool_size=(2, 2)),\n",
        "    Dropout(0.4),\n",
        "\n",
        "    Flatten(),\n",
        "    Dense(1024, activation='relu'),\n",
        "    Dropout(0.5),\n",
        "    Dense(10, activation='softmax')\n",
        "])\n",
        "\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "datagen = ImageDataGenerator(\n",
        "    rotation_range=10,\n",
        "    width_shift_range=0.05,\n",
        "    height_shift_range=0.05,\n",
        "    horizontal_flip=True,\n",
        "    fill_mode='nearest',\n",
        "    zoom_range=0.1\n",
        ")\n",
        "datagen.fit(X_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OG9IT7u_wOOm"
      },
      "source": [
        "Configuration 9 - Improving on Configuration 7 (Batch Normalization, Learning Rate Scheduler)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "XSJ25TjRwOOm"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.layers import BatchNormalization\n",
        "from tensorflow.keras.regularizers import l2\n",
        "\n",
        "# Adjusting the model by adding Batch Normalization\n",
        "model9 = Sequential([\n",
        "    Conv2D(64, (3, 3), padding='same', activation='relu', kernel_regularizer=l2(0.001), input_shape=(32, 32, 3)),\n",
        "    BatchNormalization(),\n",
        "    Conv2D(64, (3, 3), padding='same', activation='relu', kernel_regularizer=l2(0.001)),\n",
        "    BatchNormalization(),\n",
        "    MaxPooling2D(pool_size=(2, 2)),\n",
        "    Dropout(0.3),\n",
        "\n",
        "    Conv2D(128, (3, 3), padding='same', activation='relu', kernel_regularizer=l2(0.001)),\n",
        "    BatchNormalization(),\n",
        "    Conv2D(128, (3, 3), padding='same', activation='relu', kernel_regularizer=l2(0.001)),\n",
        "    BatchNormalization(),\n",
        "    MaxPooling2D(pool_size=(2, 2)),\n",
        "    Dropout(0.4),\n",
        "\n",
        "    Flatten(),\n",
        "    Dense(1024, activation='relu'),\n",
        "    Dropout(0.5),\n",
        "    Dense(10, activation='softmax')\n",
        "])\n",
        "\n",
        "# Implementing a Learning Rate Scheduler\n",
        "from keras.callbacks import LearningRateScheduler\n",
        "from keras.optimizers import Adam\n",
        "\n",
        "def scheduler(epoch, lr):\n",
        "    if epoch < 10:\n",
        "        return float(lr)\n",
        "    else:\n",
        "        return float(lr * tf.math.exp(-0.1))\n",
        "\n",
        "lr_schedule = LearningRateScheduler(scheduler)\n",
        "optimizer = Adam(learning_rate=0.001)\n",
        "\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "datagen = ImageDataGenerator(\n",
        "    rotation_range=10,\n",
        "    width_shift_range=0.05,\n",
        "    height_shift_range=0.05,\n",
        "    horizontal_flip=True,\n",
        "    fill_mode='nearest',\n",
        "    zoom_range=0.1\n",
        ")\n",
        "datagen.fit(X_train)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "\n",
        "model8 = model7\n",
        "models = [model1, model2, model3, model4, model5, model6, model7, model8, model9]\n",
        "\n",
        "test_accuracies = {}\n",
        "\n",
        "for model in models:\n",
        "    model_num = models.index(model) + 1\n",
        "    learning_rate = 0.001  # Default learning rate\n",
        "    if 1 <= model_num <= 3 or model_num == 5:\n",
        "        learning_rate = 0.0001\n",
        "    elif model_num == 4 or model_num == 6 or model_num == 7:\n",
        "        learning_rate = 0.001\n",
        "    elif model_num == 8:\n",
        "        learning_rate = 0.01\n",
        "\n",
        "    # Reinitialize the optimizer with the specific learning rate for each model\n",
        "    optimizer = Adam(learning_rate=learning_rate)\n",
        "    model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "    checkpoint = ModelCheckpoint(f'model{model_num}.keras', save_best_only=True)\n",
        "    early_stopping = EarlyStopping(monitor='val_loss', verbose=1, patience=10, restore_best_weights=True)\n",
        "    epochs = 50 if model_num <= 6 else 100\n",
        "\n",
        "    # Decide the fitting strategy based on model number\n",
        "    if model_num == 9:\n",
        "        history = model.fit(X_train, Y_train, batch_size=64, epochs=epochs, validation_data=(X_test, Y_test), callbacks=[checkpoint, early_stopping])\n",
        "    else:\n",
        "        history = model.fit(datagen.flow(X_train, Y_train, batch_size=64), epochs=epochs, validation_data=(X_test, Y_test), callbacks=[checkpoint, early_stopping])\n",
        "\n",
        "    # Evaluate model on test data\n",
        "    scores = model.evaluate(X_test, Y_test, verbose=1)\n",
        "    print(f'Model {model_num} Test accuracy: {scores[1]*100}%')\n",
        "    test_accuracies[model_num] = scores[1] * 100\n",
        "\n",
        "    # Predict the outputs on the test set\n",
        "    Y_pred = model.predict(X_test)\n",
        "    Y_pred_classes = np.argmax(Y_pred, axis=1)\n",
        "    Y_true = np.argmax(Y_test, axis=1)\n",
        "\n",
        "    '''\n",
        "\n",
        "    # Generate the confusion matrix\n",
        "    cm = confusion_matrix(Y_true, Y_pred_classes)\n",
        "    # Optionally visualize the confusion matrix\n",
        "    plt.figure(figsize=(10, 8))\n",
        "    sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\")\n",
        "    plt.ylabel('True Label')\n",
        "    plt.xlabel('Predicted Label')\n",
        "    plt.title(f'Confusion Matrix for Model {model_num}')\n",
        "    plt.savefig(f'confusion_matrix_model{model_num}.png')\n",
        "    plt.close()\n",
        "    '''\n",
        "\n",
        "\n",
        "pd.DataFrame(test_accuracies, index=['Test Accuracy']).to_csv('test_accuracies.csv')\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RavgebIEw-iJ",
        "outputId": "2fa64a8b-d73b-4e5a-e868-5ac5b1913a8b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "782/782 [==============================] - 28s 33ms/step - loss: 1.5710 - accuracy: 0.4255 - val_loss: 1.4313 - val_accuracy: 0.4795\n",
            "Epoch 2/50\n",
            "782/782 [==============================] - 26s 34ms/step - loss: 1.4825 - accuracy: 0.4631 - val_loss: 1.3448 - val_accuracy: 0.5196\n",
            "Epoch 3/50\n",
            "782/782 [==============================] - 26s 33ms/step - loss: 1.4151 - accuracy: 0.4888 - val_loss: 1.2780 - val_accuracy: 0.5404\n",
            "Epoch 4/50\n",
            "782/782 [==============================] - 26s 33ms/step - loss: 1.3595 - accuracy: 0.5123 - val_loss: 1.2678 - val_accuracy: 0.5436\n",
            "Epoch 5/50\n",
            "696/782 [=========================>....] - ETA: 2s - loss: 1.3119 - accuracy: 0.5315"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model8 = model7\n",
        "optimizer = Adam(learning_rate=0.01)\n",
        "model8.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "checkpoint = ModelCheckpoint('model8.keras', save_best_only=True)\n",
        "early_stopping = EarlyStopping(monitor='val_loss', verbose=1, patience=10, restore_best_weights=True)\n",
        "epochs = 50\n",
        "history = model8.fit(datagen.flow(X_train, Y_train, batch_size=64), epochs=epochs, validation_data=(X_test, Y_test), callbacks=[checkpoint, early_stopping])\n",
        "# Evaluate model on test data\n",
        "scores = model8.evaluate(X_test, Y_test, verbose=1)\n",
        "print(f'Model 8 Test accuracy: {scores[1]*100}%')\n",
        "# Predict the outputs on the test set\n",
        "Y_pred = model8.predict(X_test)\n",
        "Y_pred_classes = np.argmax(Y_pred, axis=1)\n",
        "Y_true = np.argmax(Y_test, axis=1)\n",
        "\n",
        "# Generate the confusion matrix\n",
        "cm = confusion_matrix(Y_true, Y_pred_classes)\n",
        "# Optionally visualize the confusion matrix\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\")\n",
        "plt.ylabel('True Label')\n",
        "plt.xlabel('Predicted Label')\n",
        "plt.title(f'Confusion Matrix for Model 8')\n",
        "plt.savefig(f'confusion_matrix_model8.png')\n",
        "plt.close()"
      ],
      "metadata": {
        "id": "Sqd8pu3wYVR4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from google.colab import files\n",
        "filename = f'confusion_matrix_model{8}.png'\n",
        "files.download(filename)"
      ],
      "metadata": {
        "id": "DlgHmT7sLWTD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model1.summary()\n",
        "model2.summary()\n",
        "model3.summary()\n",
        "model4.summary()\n",
        "model5.summary()\n",
        "model6.summary()\n",
        "model7.summary()\n",
        "model8.summary()\n",
        "model9.summary()"
      ],
      "metadata": {
        "id": "XnarnjcYfkal"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.2"
    },
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}