{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "L4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "def preprocess_image(image_path):\n",
        "    # Util function to open, resize and format pictures into appropriate tensors\n",
        "    img = keras.utils.load_img(image_path, target_size=(img_nrows, img_ncols))\n",
        "    img = keras.utils.img_to_array(img)\n",
        "    img = np.expand_dims(img, axis=0)\n",
        "    img = vgg19.preprocess_input(img)\n",
        "    return tf.convert_to_tensor(img)\n",
        "\n",
        "\n",
        "def deprocess_image(x):\n",
        "    # Util function to convert a tensor into a valid image\n",
        "    x = x.reshape((img_nrows, img_ncols, 3))\n",
        "    # Remove zero-center by mean pixel\n",
        "    x[:, :, 0] += 103.939\n",
        "    x[:, :, 1] += 116.779\n",
        "    x[:, :, 2] += 123.68\n",
        "    # 'BGR'->'RGB'\n",
        "    x = x[:, :, ::-1]\n",
        "    x = np.clip(x, 0, 255).astype(\"uint8\")\n",
        "    return x\n"
      ],
      "metadata": {
        "id": "sVdUi4qhyty3"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# The gram matrix of an image tensor (feature-wise outer product)\n",
        "\n",
        "\n",
        "def gram_matrix(x):\n",
        "    x = tf.transpose(x, (2, 0, 1))\n",
        "    features = tf.reshape(x, (tf.shape(x)[0], -1))\n",
        "    gram = tf.matmul(features, tf.transpose(features))\n",
        "    return gram\n",
        "\n",
        "\n",
        "# The \"style loss\" is designed to maintain\n",
        "# the style of the reference image in the generated image.\n",
        "# It is based on the gram matrices (which capture style) of\n",
        "# feature maps from the style reference image\n",
        "# and from the generated image\n",
        "\n",
        "\n",
        "def style_loss(style, combination):\n",
        "    S = gram_matrix(style)\n",
        "    C = gram_matrix(combination)\n",
        "    channels = 3\n",
        "    size = img_nrows * img_ncols\n",
        "    return tf.reduce_sum(tf.square(S - C)) / (4.0 * (channels**2) * (size**2))\n",
        "\n",
        "\n",
        "# An auxiliary loss function\n",
        "# designed to maintain the \"content\" of the\n",
        "# base image in the generated image\n",
        "\n",
        "\n",
        "def content_loss(base, combination):\n",
        "    return tf.reduce_sum(tf.square(combination - base))\n",
        "\n",
        "\n",
        "# The 3rd loss function, total variation loss,\n",
        "# designed to keep the generated image locally coherent\n",
        "\n",
        "\n",
        "def total_variation_loss(x):\n",
        "    a = tf.square(\n",
        "        x[:, : img_nrows - 1, : img_ncols - 1, :] - x[:, 1:, : img_ncols - 1, :]\n",
        "    )\n",
        "    b = tf.square(\n",
        "        x[:, : img_nrows - 1, : img_ncols - 1, :] - x[:, : img_nrows - 1, 1:, :]\n",
        "    )\n",
        "    return tf.reduce_sum(tf.pow(a + b, 1.25))\n"
      ],
      "metadata": {
        "id": "vNtVPd3DyzQK"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Build a VGG19 model loaded with pre-trained ImageNet weights\n",
        "model = vgg19.VGG19(weights=\"imagenet\", include_top=False)\n",
        "\n",
        "# Get the symbolic outputs of each \"key\" layer (we gave them unique names).\n",
        "outputs_dict = dict([(layer.name, layer.output) for layer in model.layers])\n",
        "\n",
        "# Set up a model that returns the activation values for every layer in\n",
        "# VGG19 (as a dict).\n",
        "feature_extractor = keras.Model(inputs=model.inputs, outputs=outputs_dict)"
      ],
      "metadata": {
        "id": "dvN1MFxFy23D"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# List of layers to use for the style loss.\n",
        "style_layer_names = [\n",
        "    \"block1_conv1\",\n",
        "    \"block2_conv1\",\n",
        "    \"block3_conv1\",\n",
        "    \"block4_conv1\",\n",
        "    \"block5_conv1\",\n",
        "]\n",
        "# The layer to use for the content loss.\n",
        "content_layer_name = \"block5_conv2\"\n",
        "\n",
        "\n",
        "def compute_loss(combination_image, base_image, style_reference_image):\n",
        "    input_tensor = tf.concat(\n",
        "        [base_image, style_reference_image, combination_image], axis=0\n",
        "    )\n",
        "    features = feature_extractor(input_tensor)\n",
        "\n",
        "    # Initialize the loss\n",
        "    loss = tf.zeros(shape=())\n",
        "\n",
        "    # Add content loss\n",
        "    layer_features = features[content_layer_name]\n",
        "    base_image_features = layer_features[0, :, :, :]\n",
        "    combination_features = layer_features[2, :, :, :]\n",
        "    loss = loss + content_weight * content_loss(\n",
        "        base_image_features, combination_features\n",
        "    )\n",
        "    # Add style loss\n",
        "    for layer_name in style_layer_names:\n",
        "        layer_features = features[layer_name]\n",
        "        style_reference_features = layer_features[1, :, :, :]\n",
        "        combination_features = layer_features[2, :, :, :]\n",
        "        sl = style_loss(style_reference_features, combination_features)\n",
        "        loss += (style_weight / len(style_layer_names)) * sl\n",
        "\n",
        "    # Add total variation loss\n",
        "    loss += total_variation_weight * total_variation_loss(combination_image)\n",
        "    return loss\n"
      ],
      "metadata": {
        "id": "RQzo1mnWzA3V"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@tf.function\n",
        "def compute_loss_and_grads(combination_image, base_image, style_reference_image):\n",
        "    with tf.GradientTape() as tape:\n",
        "        loss = compute_loss(combination_image, base_image, style_reference_image)\n",
        "    grads = tape.gradient(loss, combination_image)\n",
        "    return loss, grads"
      ],
      "metadata": {
        "id": "B9ZjikzQzJmA"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g1umhsGhMcpR",
        "outputId": "d1901479-f9a1-4e7e-bc2f-58d519f6ee88"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 100: loss=2058.96\n",
            "Iteration 200: loss=1247.31\n",
            "Iteration 300: loss=978.76\n",
            "Iteration 400: loss=850.17\n",
            "Iteration 500: loss=775.63\n",
            "Iteration 600: loss=726.76\n",
            "Iteration 700: loss=692.00\n",
            "Iteration 800: loss=665.94\n",
            "Iteration 900: loss=645.47\n",
            "Iteration 1000: loss=628.97\n",
            "Iteration 1100: loss=615.33\n",
            "Iteration 1200: loss=603.83\n",
            "Iteration 1300: loss=593.99\n",
            "Iteration 1400: loss=585.50\n",
            "Iteration 1500: loss=578.08\n",
            "Iteration 1600: loss=571.51\n",
            "Iteration 1700: loss=565.65\n",
            "Iteration 1800: loss=560.41\n",
            "Iteration 1900: loss=555.70\n",
            "Iteration 2000: loss=551.44\n",
            "Iteration 2100: loss=547.57\n",
            "Iteration 2200: loss=544.04\n",
            "Iteration 2300: loss=540.83\n",
            "Iteration 2400: loss=537.87\n",
            "Iteration 2500: loss=535.14\n",
            "Iteration 2600: loss=532.61\n",
            "Iteration 2700: loss=530.26\n",
            "Iteration 2800: loss=528.08\n",
            "Iteration 2900: loss=526.05\n",
            "Iteration 3000: loss=524.16\n",
            "Iteration 3100: loss=522.40\n",
            "Iteration 3200: loss=520.75\n",
            "Iteration 3300: loss=519.21\n",
            "Iteration 3400: loss=517.76\n",
            "Iteration 3500: loss=516.40\n",
            "Iteration 3600: loss=515.12\n",
            "Iteration 3700: loss=513.91\n",
            "Iteration 3800: loss=512.78\n",
            "Iteration 3900: loss=511.71\n",
            "Iteration 4000: loss=510.70\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "os.environ[\"KERAS_BACKEND\"] = \"tensorflow\"\n",
        "\n",
        "import keras\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from keras.applications import vgg19\n",
        "from PIL import Image\n",
        "\n",
        "base_image_path = '/content/drive/MyDrive/2024-2025/Deep_Learning_Assignments/Datasets/custom_images/'\n",
        "style_ref_path = '/content/style_refs/'\n",
        "images = {'image1': base_image_path + 'image1_me_and_sis.jpg',\n",
        "          'image2': base_image_path + 'image2_me_dad_sis.jpg',\n",
        "          'image3': base_image_path + 'image3_iguazu_falls.jpg',\n",
        "          'image4': base_image_path + 'image4_purple_flowers.jpg',\n",
        "          'image5': base_image_path + 'image5_mural.jpg',\n",
        "          'image6': base_image_path + 'image6_cake.jpg',\n",
        "          'image7': base_image_path + 'image7_lucky_dog.jpg'}\n",
        "style_refs = {'image1': style_ref_path + 'scream.jpg',\n",
        "              'image2': style_ref_path + 'scream.jpg',\n",
        "              'image3': style_ref_path + 'starry_night.jpg',\n",
        "              'image4': style_ref_path + 'wave.jpg',\n",
        "              'image5': style_ref_path + 'starry_night.jpg',\n",
        "              'image6': style_ref_path + 'colorful.jpg',\n",
        "              'image7': style_ref_path + 'black_white.jpg'}\n",
        "\n",
        "for image_name, image_path in images.items():\n",
        "  for style_ref_name, style_ref_path in style_refs.items():\n",
        "    if image_name == style_ref_name and ( image_name == 'image7'):\n",
        "      style_reference_image_path = style_ref_path\n",
        "      result_prefix = image_name + '_generated'\n",
        "      total_variation_weight = 1e-6\n",
        "      style_weight = 1e-6\n",
        "      content_weight = 2.5e-8\n",
        "      width, height = keras.utils.load_img(images[image_name]).size\n",
        "      img_nrows = 400\n",
        "      img_ncols = int(width * img_nrows / height)\n",
        "      img_path = images[image_name]\n",
        "      img = Image.open(img_path)\n",
        "      style_image = Image.open(style_reference_image_path)\n",
        "      img_resized = img.resize(style_image.size)\n",
        "      img_resized.save(img_path, 'JPEG')\n",
        "      optimizer = keras.optimizers.SGD(\n",
        "          keras.optimizers.schedules.ExponentialDecay(initial_learning_rate=100.0, decay_steps=100, decay_rate=0.96)\n",
        "          )\n",
        "      base_image = preprocess_image(img_path)\n",
        "      style_reference_image = preprocess_image(style_reference_image_path)\n",
        "      combination_image = tf.Variable(preprocess_image(img_path))\n",
        "      iterations = 4000\n",
        "      for i in range(1, iterations + 1):\n",
        "        loss, grads = compute_loss_and_grads(\n",
        "          combination_image, base_image, style_reference_image\n",
        "        )\n",
        "        optimizer.apply_gradients([(grads, combination_image)])\n",
        "        if i % 100 == 0:\n",
        "          print(\"Iteration %d: loss=%.2f\" % (i, loss))\n",
        "          img = deprocess_image(combination_image.numpy())\n",
        "          fname = result_prefix + \"_at_iteration_%d.png\" % i\n",
        "          keras.utils.save_img(f'/content/results/{image_name}/results_during_optimization/' + fname, img)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import zipfile\n",
        "import os\n",
        "\n",
        "def zipdir(path, ziph):\n",
        "    # ziph is zipfile handle\n",
        "    for root, dirs, files in os.walk(path):\n",
        "        for file in files:\n",
        "            ziph.write(os.path.join(root, file),\n",
        "                       os.path.relpath(os.path.join(root, file),\n",
        "                                       os.path.join(path, '..')))\n",
        "\n",
        "# Specifying the directory\n",
        "results_directory = '/content/results'\n",
        "style_refs_directory = '/content/style_refs'\n",
        "\n",
        "\n",
        "# Writing files to a zipfile\n",
        "with zipfile.ZipFile('results.zip', 'w', zipfile.ZIP_DEFLATED) as zipf:\n",
        "    zipdir(results_directory, zipf)\n",
        "with zipfile.ZipFile('style_refs.zip', 'w', zipfile.ZIP_DEFLATED) as zipf:\n",
        "    zipdir(style_refs_directory, zipf)"
      ],
      "metadata": {
        "id": "a3fF6int_2b-"
      },
      "execution_count": 33,
      "outputs": []
    }
  ]
}